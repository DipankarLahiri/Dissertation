---
title: "Analysis of News Headlines"
author: "Dipankar Lahiri"
date: "2025-07-15"
output: html_document
---

# Data Loading

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readxl)
library(lubridate)
library(dplyr)
library(tidyr) 
library(scales)
library(plotly)
library(ggplot2)
library(htmlwidgets)
library(broom)
library(splines)
library(effects)
library(mgcv)
library(purrr)
library(logistf)
library(car)
library(FactoMineR)
library(factoextra)
library(pheatmap)

df <- read_excel("Headlines 22 Apr to 16 May.xlsx", sheet = "Emotions")  

glimpse(df)                   
sum(is.na(df$Headline))  
df <- df %>% mutate(Date = as.Date(Date))

```

# EMOTION - Basic prevalence checks - Emotion means across dataset, emotion means across Twitter vs Website

```{r}

emotion_cols <- c("Anger","Fear","Joy","Sadness","Trust","Surprise",
                  "Disgust","Anticipation","Nostalgia","Pride","Shame","Vindication")

# Pivot longer - Collapse 12 emotion columns into two new columns: Emotion, Score.

df_long <- df %>% 
  pivot_longer(cols = all_of(emotion_cols),
               names_to = "Emotion",
               values_to = "Score")

emotion_means <- df_long %>% 
  group_by(Emotion) %>% 
  summarise(mean_score = mean(Score, na.rm = TRUE)) %>% 
  arrange(desc(mean_score))

df <- df %>%
  mutate(SourceType = case_when(
    str_starts(Source, "Twitter") ~ "Twitter",
    TRUE ~ "Website"
  ))

emotion_by_source <- df_long %>%
  mutate(SourceType = case_when(
    str_starts(Source, "Twitter") ~ "Twitter",
    TRUE ~ "Website"
  )) %>%
  group_by(SourceType, Emotion) %>%
  summarise(mean_score = mean(Score, na.rm = TRUE)) %>%
  pivot_wider(names_from = Emotion, values_from = mean_score)

```

# EMOTION + TIME - Emotional timeline

```{r, fig.width=12, fig.height=8}

df_long %>% 
  group_by(Date, Emotion) %>% 
  summarise(day_score = mean(Score), .groups = "drop") %>% 
  ggplot(aes(Date, day_score, colour = Emotion)) +
  geom_line() +
  facet_wrap(~Emotion, scales = "free_y") +
  scale_x_date(date_breaks = "3 days",      # one tick every 3rd day
               date_labels = "%d‑%b") +     # e.g. 22‑Apr
  theme_minimal(base_size = 10) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")

```

```{r, echo = FALSE, message = FALSE}

# 1. Make and save a high‑res PNG for each emotion
walk(emotion_cols, function(em){

  p <- df_long %>% 
    filter(Emotion == em) %>% 
    group_by(Date) %>% 
    summarise(day_score = mean(Score, na.rm = TRUE), .groups = "drop") %>% 
    ggplot(aes(Date, day_score)) +
      geom_line(colour = "#2c7fb8", linewidth = 0.8) +
      labs(title = em, x = NULL, y = "Mean daily score") +
      theme_minimal(base_size = 12)

  ggsave(filename = paste0("emotion_", em, ".png"),
         plot = p, width = 6, height = 4, dpi = 300)
})

# 2. Build one interactive dashboard (12 sub‑plots) and save to HTML
plot_list <- map(emotion_cols, function(em){

  p <- df_long %>% 
    filter(Emotion == em) %>% 
    group_by(Date) %>% 
    summarise(day_score = mean(Score, na.rm = TRUE), .groups = "drop") %>% 
    ggplot(aes(Date, day_score)) +
      geom_line(colour = "#e34a33") +
      labs(title = em, x = NULL, y = NULL) +
      theme_minimal(base_size = 10)

  ggplotly(p) %>% layout(showlegend = FALSE)
})

interactive_grid <- subplot(plot_list, nrows = 4, shareX = TRUE, shareY = FALSE)
saveWidget(interactive_grid, "emotion_dashboard.html", selfcontained = TRUE)

```

# EMOTION + ENGAGEMENT - Engagement metrics

```{r}

eng_df <- df %>% 
  filter(!is.na(Views) | !is.na(Retweets) | !is.na(Likes) | !is.na(Replies))

# Check distribution with histograms for one emotion and one engagement metric as example
ggplot(eng_df, aes(x = Anger)) + geom_histogram(bins = 30) + ggtitle("Anger distribution")
ggplot(eng_df, aes(x = Views)) + geom_histogram(bins = 30) + ggtitle("Views distribution")

# Shapiro-Wilk normality test for a sample variable (only works well on <5000 samples)
shapiro.test(df$Anger)
shapiro.test(df$Views)

```

# Both p-values are tiny (< 0.05), which means neither Anger nor Views is normally distributed. Therefore, using Spearman correlation throughout engagement vs emotion analysis. It’s nonparametric and handles skewed data better.

```{r}

emotion_vs_eng <- map_dfr(emotion_cols, function(e) {
  map_dfr(c("Views", "Retweets", "Likes", "Replies"), function(kpi) {
    corr_val <- cor(eng_df[[e]], eng_df[[kpi]], use = "pairwise.complete.obs", method = "spearman")
    tibble(Emotion = e, KPI = kpi, corr = corr_val)
  })
})

# Sort by absolute correlation descending
emotion_vs_eng_sorted <- emotion_vs_eng %>% arrange(desc(abs(corr)))

print(emotion_vs_eng_sorted)


```

```{r}

cor.test(eng_df$Trust, eng_df$Views, method = "spearman")
cor.test(eng_df$Shame, eng_df$Views, method = "spearman")
cor.test(eng_df$Sadness, eng_df$Retweets, method = "spearman")
cor.test(eng_df$Disgust, eng_df$Retweets, method = "spearman")

```

```{r, message=FALSE}

# ----- 0. Define cutoffs and create binary flags for high engagement -----
view_cut  <- quantile(eng_df$Views, 0.9, na.rm = TRUE)
like_cut  <- quantile(eng_df$Likes, 0.9, na.rm = TRUE)
retweet_cut <- quantile(eng_df$Retweets, 0.9, na.rm = TRUE)
reply_cut <- quantile(eng_df$Replies, 0.9, na.rm = TRUE)

eng_df <- eng_df %>% 
  mutate(
    high_views = as.integer(Views >= view_cut),
    high_likes = as.integer(Likes >= like_cut),
    high_retweets = as.integer(Retweets >= retweet_cut),
    high_replies = as.integer(Replies >= reply_cut)
  )

# ----- 1. Logistic model formula -----
form <- as.formula(
  paste("high_views ~ SourceType +", paste(emotion_cols, collapse = " + "))
)

# ----- 2. Fit models for each engagement metric -----
models <- list(
  views = glm(high_views ~ SourceType + ., data = select(eng_df, high_views, SourceType, all_of(emotion_cols)), family = binomial),
  likes = glm(high_likes ~ SourceType + ., data = select(eng_df, high_likes, SourceType, all_of(emotion_cols)), family = binomial),
  retweets = glm(high_retweets ~ SourceType + ., data = select(eng_df, high_retweets, SourceType, all_of(emotion_cols)), family = binomial),
  replies = glm(high_replies ~ SourceType + ., data = select(eng_df, high_replies, SourceType, all_of(emotion_cols)), family = binomial)
)

# ----- 3. Tidy and combine results -----
results <- map_df(names(models), function(metric) {
  tidy(models[[metric]], exponentiate = TRUE) %>%
    mutate(
      Metric = metric,
      estimate = round(estimate, 3),
      p.value = signif(p.value, 3)
    )
})

print(results)


```

```{r}

# 1. z‑score each metric and build a composite
eng_df <- eng_df %>%
  mutate(
    across(c(Views, Likes, Retweets, Replies), scale, .names = "{.col}_z"),
    EngagedScore = rowMeans(across(ends_with("_z")), na.rm = TRUE)
  )

# 2. flag the top 10 % composite as 'high engagement'
cutoff <- quantile(eng_df$EngagedScore, 0.90, na.rm = TRUE)
eng_df <- eng_df %>% mutate(high_engage = as.integer(EngagedScore >= cutoff))

# 3. run a logistic model with emotions (+ SourceType as control)
form <- as.formula(
  paste("high_engage ~ SourceType +", paste(emotion_cols, collapse = " + "))
)
model_composite <- glm(form, data = eng_df, family = binomial)

broom::tidy(model_composite, exponentiate = TRUE)


```

# EMOTION + TIME + ENGAGEMENT

```{r}

# Day index (keep this)
eng_df <- eng_df %>% mutate(Day = as.numeric(Date - min(Date) + 1))

gam_mod <- gam(
  EngagedScore ~ s(Day) +                              # baseline time curve
    s(Day, by = Surprise) +                            # emotion‑specific curves
    s(Day, by = Disgust)  +
    s(Day, by = Nostalgia) +
    s(Day, by = Pride)   +
    SourceType +
    Anger + Fear + Joy + Sadness + Trust + Anticipation + Shame + Vindication,
  data   = eng_df,
  method = "REML"
)

summary(gam_mod)      # view EDF and p‑values for each s(Day,by=Emotion) term
plot(gam_mod, pages = 1)   # smooth curves with CIs


```

```{r}

model_emotions <- glm(high_engage ~ SourceType + Anger + Fear + Joy + Sadness + Trust + Surprise + Disgust + Anticipation + Nostalgia + Pride + Shame + Vindication, data = eng_df, family = binomial)

# Assuming your data frame is `eng_df` and emotion_cols contains your 12 emotion variable names

# 1. Filter to complete cases on emotion columns
emotion_data <- eng_df %>% select(all_of(emotion_cols)) %>% na.omit()

# 2. Run PCA (scaled)
pca_emotions <- PCA(emotion_data, scale.unit = TRUE, graph = FALSE)

# 3. Scree plot to decide number of components
fviz_screeplot(pca_emotions, addlabels = TRUE, ylim = c(0, 50))

# 4. Extract variable loadings (which emotions load on which PCs)
loadings <- pca_emotions$var$coord %>% as.data.frame() %>% rownames_to_column(var = "Emotion")
print(loadings)

# 5. Bind PCA scores (component coordinates) back to your filtered data for modeling
pc_scores <- as.data.frame(pca_emotions$ind$coord)

# If you want to keep the engagement variable aligned
eng_df_pca <- bind_cols(eng_df %>% filter(complete.cases(select(., all_of(emotion_cols)))), pc_scores)

# 6. Run logistic regression on first few PCs predicting high engagement
model_pca <- glm(high_engage ~ Dim.1 + Dim.2 + Dim.3, data = eng_df_pca, family = binomial)
summary(model_pca)

```

# Themes

```{r theme-load-and-time, message=FALSE}

# ---- 0. Load the themes sheet and merge with existing 'df' ----
theme_cols <- c("Nationalism","Religion","Violence","Conspiracy",
                "Political figures","Statistics","History","Global Affairs",
                "Hero worship","Gender","Humour","Bullying")

themes_raw <- read_excel("Headlines 22 Apr to 16 May.xlsx",
                         sheet = "Themes") %>%
              select(Headline, all_of(theme_cols))   # keep only headline + themes

df <- df %>%
      left_join(themes_raw, by = "Headline")

theme_means <- df %>%
  summarise(across(all_of(theme_cols), mean, na.rm = TRUE)) %>%
  pivot_longer(cols = everything(),
               names_to = "Theme",
               values_to = "MeanScore") %>%
  arrange(desc(MeanScore))

print(theme_means)


```
# Theme + Time

```{r}

# ---- 3. Theme timeline (daily mean score per theme) ----
df %>%
  pivot_longer(cols = all_of(theme_cols),
               names_to = "Theme",
               values_to = "Score") %>%
  group_by(Date, Theme) %>%
  summarise(day_score = mean(Score), .groups = "drop") %>%
  ggplot(aes(Date, day_score, colour = Theme)) +
  geom_line() +
  facet_wrap(~Theme, scales = "free_y", ncol = 3) +
  scale_x_date(date_breaks = "3 days", date_labels = "%d‑%b") +
  theme_minimal(base_size = 10) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")


```
```{r}

# 1. Save PNGs per theme
walk(theme_cols, function(th){
  p <- df %>%
    pivot_longer(cols = all_of(theme_cols),
                 names_to = "Theme",
                 values_to = "Score") %>%
    filter(Theme == th) %>%
    group_by(Date) %>%
    summarise(day_score = mean(Score, na.rm = TRUE), .groups = "drop") %>%
    ggplot(aes(Date, day_score)) +
    geom_line(colour = "#2c7fb8", linewidth = 0.8) +
    labs(title = th, x = NULL, y = "Mean daily score") +
    theme_minimal(base_size = 12)

  ggsave(filename = paste0("theme_", th, ".png"),
         plot = p, width = 6, height = 4, dpi = 300)
})

# 2. Build interactive dashboard
plot_list <- map(theme_cols, function(th){
  p <- df %>%
    pivot_longer(cols = all_of(theme_cols),
                 names_to = "Theme",
                 values_to = "Score") %>%
    filter(Theme == th) %>%
    group_by(Date) %>%
    summarise(day_score = mean(Score, na.rm = TRUE), .groups = "drop") %>%
    ggplot(aes(Date, day_score)) +
    geom_line(colour = "#e34a33") +
    labs(title = th, x = NULL, y = NULL) +
    theme_minimal(base_size = 10)

  ggplotly(p) %>% layout(showlegend = FALSE)
})

interactive_grid <- subplot(plot_list, nrows = 4, shareX = TRUE, shareY = FALSE)
saveWidget(interactive_grid, "theme_dashboard.html", selfcontained = TRUE)


```


# Twitter vs Website

```{r theme-by-source, message=FALSE}

theme_long <- df %>%
  pivot_longer(cols = all_of(theme_cols),
               names_to = "Theme",
               values_to = "Score")

theme_by_source <- theme_long %>%
  group_by(Source, Theme) %>%
  summarise(mean_score = mean(Score, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = Theme, values_from = mean_score)

print(theme_by_source)

theme_by_source <- theme_long %>%
  mutate(SourceType = if_else(str_starts(Source, "Twitter"), "Twitter", "Website")) %>%
  group_by(SourceType, Theme) %>%
  summarise(mean_score = mean(Score, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = Theme, values_from = mean_score)

# Delta (Website minus Twitter)
theme_deltas <- theme_by_source %>%
  pivot_longer(-SourceType, names_to = "Theme", values_to = "mean") %>%
  pivot_wider(names_from = SourceType, values_from = mean) %>%
  mutate(delta = Website - Twitter) %>%
  arrange(desc(delta))

print(theme_deltas)

```

# Theme + Engagement Metrics

```{r}

eng_df <- themes_raw %>%
  inner_join(df %>% select(Headline, Views, Retweets, Likes, Replies, Source), by = "Headline") %>%
  mutate(SourceType = if_else(str_starts(Source, "Twitter"), "Twitter", "Website")) %>%
  filter(!is.na(Views) | !is.na(Retweets) | !is.na(Likes) | !is.na(Replies))

eng_df <- eng_df %>%
  mutate(across(c(Views, Likes, Retweets, Replies), scale, .names = "{.col}_z")) %>%
  mutate(EngagedScore = rowMeans(across(ends_with("_z")), na.rm = TRUE))

cutoff <- quantile(eng_df$EngagedScore, 0.90, na.rm = TRUE)
eng_df <- eng_df %>%
  mutate(high_engage = as.integer(EngagedScore >= cutoff))

theme_cols <- colnames(themes_raw)[-1]

theme_long <- eng_df %>%
  pivot_longer(cols = all_of(theme_cols),
               names_to = "Theme",
               values_to = "ThemeScore")

theme_vs_eng <- map_dfr(theme_cols, function(theme) {
  map_dfr(c("Views", "Retweets", "Likes", "Replies"), function(kpi) {
    corr_val <- cor(eng_df[[theme]], eng_df[[kpi]], use = "pairwise.complete.obs", method = "spearman")
    tibble(Theme = theme, KPI = kpi, corr = corr_val)
  })
}) %>%
  arrange(desc(abs(corr)))

print(theme_vs_eng)

```


```{r}

eng_df <- eng_df %>%
  mutate(
    high_views = as.integer(Views >= quantile(Views, 0.9, na.rm = TRUE)),
    high_likes = as.integer(Likes >= quantile(Likes, 0.9, na.rm = TRUE)),
    high_retweets = as.integer(Retweets >= quantile(Retweets, 0.9, na.rm = TRUE)),
    high_replies = as.integer(Replies >= quantile(Replies, 0.9, na.rm = TRUE))
  )

models <- list(
  views = glm(high_views ~ SourceType + ., data = select(eng_df, high_views, SourceType, all_of(theme_cols)), family = binomial),
  likes = glm(high_likes ~ SourceType + ., data = select(eng_df, high_likes, SourceType, all_of(theme_cols)), family = binomial),
  retweets = glm(high_retweets ~ SourceType + ., data = select(eng_df, high_retweets, SourceType, all_of(theme_cols)), family = binomial),
  replies = glm(high_replies ~ SourceType + ., data = select(eng_df, high_replies, SourceType, all_of(theme_cols)), family = binomial)
)

results <- map_df(names(models), function(metric) {
  broom::tidy(models[[metric]], exponentiate = TRUE) %>%
    mutate(
      Metric = metric,
      estimate = round(estimate, 3),
      p.value = signif(p.value, 3)
    )
})

print(results)

```


```{r}

theme_cols_backtick <- ifelse(grepl("\\s", theme_cols),
                             paste0("`", theme_cols, "`"),
                             theme_cols)

form <- as.formula(
  paste("high_engage ~ SourceType +", paste(theme_cols_backtick, collapse = " + "))
)
model_composite <- glm(form, data = eng_df, family = binomial)
print(broom::tidy(model_composite, exponentiate = TRUE))

model_composite2 <- glm(
  high_engage ~ SourceType + Nationalism + Religion + Violence + 
    Conspiracy + `Political figures` + Statistics + History + 
    `Global Affairs` + `Hero worship` + Gender + Bullying,
  data = eng_df,
  family = binomial
)

```


```{r}

eng_df <- themes_raw %>%
  inner_join(df %>% select(Headline, Views, Retweets, Likes, Replies, Source, Date), by = "Headline") %>%
  mutate(SourceType = if_else(str_starts(Source, "Twitter"), "Twitter", "Website")) %>%
  filter(!is.na(Views) | !is.na(Retweets) | !is.na(Likes) | !is.na(Replies))

eng_df <- eng_df %>% mutate(Day = as.numeric(Date - min(Date) + 1))
eng_df_clean <- eng_df %>%
  rename(
    Political_figures = `Political figures`,
    Global_Affairs = `Global Affairs`,
    Hero_worship = `Hero worship`
  )
eng_df_clean <- eng_df_clean %>%
  mutate(across(c(Views, Likes, Retweets, Replies), scale, .names = "{.col}_z")) %>%
  mutate(EngagedScore = rowMeans(across(ends_with("_z")), na.rm = TRUE))

gam_mod <- mgcv::gam(
  EngagedScore ~ s(Day) +
    s(Day, by = Nationalism) + s(Day, by = Religion) + s(Day, by = Violence) + s(Day, by = Conspiracy) +
    s(Day, by = Political_figures) + s(Day, by = History) + s(Day, by = Global_Affairs) +
    s(Day, by = Hero_worship) + s(Day, by = Gender) +
    SourceType,
  data = eng_df_clean,
  method = "REML"
)

summary(gam_mod)
plot(gam_mod, pages = 1)

png("gam_plots.png", width = 1600, height = 2000, res = 200)
par(mfrow = c(4, 3))
plot(gam_mod, shade = TRUE)
dev.off()

```

```{r}

# Extract full summary
gam_summary <- summary(gam_mod)

# Parametric coefficients (e.g., SourceTypeTwitter)
gam_summary$p.table

# Smooth terms: edf, ref.df, F, p-value
gam_summary$s.table

# Adjusted R²
gam_summary$r.sq  # unadjusted
gam_summary$dev.expl  # deviance explained = closer to adjusted R²

# Parametric part
parametric_terms <- tidy(gam_mod, parametric = TRUE)

# Smooth terms
smooth_terms <- tidy(gam_mod, parametric = FALSE)

# Define theme columns
theme_vars <- c("Nationalism", "Religion", "Violence", "Conspiracy", "Political_figures",
                "Statistics", "History", "Global_Affairs", "Hero_worship",
                "Gender", "Humour", "Bullying")

# Correlation matrix (pairwise complete obs)
theme_corr <- cor(eng_df_clean[, theme_vars], use = "pairwise.complete.obs")

# Optional: round and print neatly
round(theme_corr, 2)

library(corrplot)
corrplot(theme_corr, method = "color", type = "upper", tl.cex = 0.7)
```

# THEME and EMOTION

```{r}

# Define variables
emotions <- c("Anger","Fear","Joy","Sadness","Trust","Surprise",
              "Disgust","Anticipation","Nostalgia","Pride","Shame","Vindication")

themes <- c("Nationalism","Religion","Violence","Conspiracy",
            "Political figures","Statistics","History","Global Affairs",
            "Hero worship","Gender","Humour","Bullying")

# 1. Co-occurrence: mean emotion score within theme score >= 0.3
df_long <- df %>%
  select(all_of(c(themes, emotions))) %>%
  pivot_longer(cols = all_of(themes), names_to = "Theme", values_to = "ThemeScore") %>%
  filter(ThemeScore >= 0.3) %>%
  select(-ThemeScore)

cooccurrence <- df_long %>%
  pivot_longer(cols = all_of(emotions), names_to = "Emotion", values_to = "EmotionScore") %>%
  group_by(Theme, Emotion) %>%
  summarise(MeanEmotion = mean(EmotionScore, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = Emotion, values_from = MeanEmotion)

# 2. Emotion voids (MeanEmotion < 0.05)
emotion_voids <- if (ncol(cooccurrence) > 1) {
  cooccurrence %>%
    pivot_longer(-Theme, names_to = "Emotion", values_to = "MeanEmotion") %>%
    filter(!is.na(MeanEmotion) & MeanEmotion < 0.05) %>%
    arrange(MeanEmotion)
} else {
  tibble(Theme = character(), Emotion = character(), MeanEmotion = numeric())
}

# 3. Prepare engagement data subset with complete cases
eng_df <- df %>%
  filter(!is.na(Views), !is.na(Likes), !is.na(Retweets), !is.na(Replies)) %>%
  mutate(
    SourceType = if_else(str_starts(Source, "Twitter"), "Twitter", "Website"),
    Views_z = as.numeric(scale(Views)),
    Likes_z = as.numeric(scale(Likes)),
    Retweets_z = as.numeric(scale(Retweets)),
    Replies_z = as.numeric(scale(Replies))
  ) %>%
  rowwise() %>%
  mutate(
    EngagedScore = mean(c_across(c(Views_z, Likes_z, Retweets_z, Replies_z)), na.rm = TRUE)
  ) %>%
  ungroup()

threshold <- quantile(eng_df$EngagedScore, 0.9, na.rm = TRUE)

eng_df <- eng_df %>%
  mutate(
    high_engage = as.integer(EngagedScore >= threshold)
  )

# 4. Logistic regression per theme predicting high_engage (Firth's correction)
theme_logistic_results <- lapply(themes, function(t) {
  t_safe <- ifelse(grepl("\\W", t), paste0("`", t, "`"), t)
  formula <- as.formula(paste("high_engage ~ SourceType +", t_safe))
  
  fit <- tryCatch({
    logistf(formula, data = eng_df, control = logistf.control(maxit = 1000, maxstep = 10))
  }, error = function(e) NULL)
  
  if (is.null(fit)) {
    tibble(Theme = t, OR = NA_real_, CI_lower = NA_real_, CI_upper = NA_real_, p_value = NA_real_)
  } else {
    est <- coef(fit)[t_safe]
    se <- sqrt(diag(vcov(fit)))[t_safe]
    p_val <- fit$prob[t_safe]
    
    tibble(
      Theme = t,
      OR = exp(est),
      CI_lower = exp(est - 1.96 * se),
      CI_upper = exp(est + 1.96 * se),
      p_value = p_val
    )
  }
}) %>% bind_rows() %>% arrange(desc(OR))

# 5. Heatmap visualization of co-occurrence (mean emotion per theme)
co_matrix <- cooccurrence %>%
  column_to_rownames(var = "Theme") %>%
  as.matrix()

co_matrix[is.na(co_matrix)] <- 0

pheatmap(co_matrix,  
         cluster_rows = TRUE, cluster_cols = TRUE, 
         fontsize = 10,
         main = "Mean Emotion Scores by Theme",
         color = colorRampPalette(c("white", "red"))(50),
         na_col = "grey90")

# 6. Print results
print(cooccurrence)
print(emotion_voids)
print(theme_logistic_results)

```

```{r}

# Threshold for top 1%
threshold <- quantile(eng_df$EngagedScore, 0.99, na.rm = TRUE)

top_headlines <- eng_df %>%
  filter(EngagedScore >= threshold) %>%
  select(Headline, all_of(themes), all_of(emotions))

# Convert themes to long format (flag if >= 0.3)
top_themes_long <- top_headlines %>%
  pivot_longer(cols = all_of(themes), names_to = "Theme", values_to = "ThemeScore") %>%
  filter(ThemeScore >= 0.3) %>%
  select(-ThemeScore)

# Convert emotions to long format
top_emotions_long <- top_headlines %>%
  pivot_longer(cols = all_of(emotions), names_to = "Emotion", values_to = "EmotionScore") %>%
  filter(EmotionScore > 0) %>%  # Adjust cutoff as needed
  select(-EmotionScore)

# Join themes and emotions by Headline to get co-occurrences
top_theme_emotion <- inner_join(top_themes_long, top_emotions_long, by = "Headline")

# Count theme-emotion pairs frequency
theme_emotion_counts <- top_theme_emotion %>%
  count(Theme, Emotion, sort = TRUE)

# Output:
list(
  theme_emotion_counts = theme_emotion_counts,
  headlines_with_pairs = top_theme_emotion %>% select(Headline, Theme, Emotion)
)

```
# WAR v PEACE

```{r}

# Load peacetime data
peacetime <- readxl::read_excel("../../Peacetime_with_scores_emotions_themes.xlsx")

# Subset just the emotion columns
emotion_cols <- c("Anger", "Fear", "Sadness", "Trust", "Surprise", "Disgust", 
                  "Anticipation", "Shame", "Vindication", "Joy", "Pride", "Nostalgia")
peacetime_emotions <- peacetime[, emotion_cols]

# Summary stats
round(colMeans(peacetime_emotions, na.rm = TRUE), 2)

# Correlation matrix
round(cor(peacetime_emotions, use = "pairwise.complete.obs"), 2)

```

```{r}

theme_cols <- c("Nationalism", "Religion", "Violence", "Conspiracy", "Political figures", 
                "Statistics", "History", "Global Affairs", "Hero worship", 
                "Gender", "Humour", "Bullying")
peacetime_themes <- peacetime[, theme_cols]

# Summary
round(colMeans(peacetime_themes, na.rm = TRUE), 2)

# Correlations
round(cor(peacetime_themes, use = "pairwise.complete.obs"), 2)

```


```{r}

# Combine for correlation matrix between emotions and themes
et_corr <- cor(peacetime_emotions, peacetime_themes, use = "pairwise.complete.obs")
round(et_corr, 2)

pheatmap(et_corr, cluster_rows = TRUE, cluster_cols = TRUE, display_numbers = TRUE)

```